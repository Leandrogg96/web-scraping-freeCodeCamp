{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37aad50-4ce8-492f-9f88-d4a877de085d",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef456117-5548-4183-929c-c5059e0afb03",
   "metadata": {},
   "source": [
    "## 1. ¿Que es el Web Scrapring?\n",
    "- Web Scraping:\n",
    "    Es la practica de recopilar datos mediante un programa automatizado que consulta un servidor web. En otras palabras, un codigo que va a realizar peticiones a un determinado servidor y extraer la informacion de un tipo de dato dentro de la pagina. El tipo de informacion a extraer puede ser de cualquier tipo como por ejemplo texto, audio, imagenes, etc.\n",
    "- Web Crawling:\n",
    "    Se utiliza para indexar la informacion de una pagina mediante bots. El objetivo de esto es entrar a todos los hipervinculos o paginas dentro de una pagina y obtener toda la informacion. Un ejemplo de esto es el bot de google que indexa las paginas y asi cuando se realice una busqueda pueda realizar la recomendacion de una pagina para dicha peticion.\n",
    "- Observacion:\n",
    "    La diferencia entre ambas es que cuando realizamos WebScraping buscamos algun tipo de dato en particular, por ejemplo si lo realizamos en una pagina de venta de ropa online, intentamos obtener unicamente la descripcion de los precios de la ropa que ofrecen, por el contrario, el WebCrawling, traeremos toda la informacion de la pagina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a69ba-e01b-4616-b827-7deeeadeb6af",
   "metadata": {},
   "source": [
    "## 2. ¿Que es una API?\n",
    "API corresponde a las siglas \"Interfaz de programacion de aplicaciones\". Es un proveedor de acceso a los datos de una aplicacion, sistema operativo u otro servicio. En ocaciones las paginas o servidores proporcionan una API con el objetivo de controlar la transaccion de la informacion solicitada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937c318-c355-4f57-9c79-fcfe0f39df68",
   "metadata": {},
   "source": [
    "## 3. API vs Web Scraping\n",
    "- Las API's son mas sencillas de utilizar a la hora de recopilar informacion de manera mas limpia y a la hora de hacer pedidos especificos.\n",
    "- Cuando querramos obtener informacion de un sitio en especifico deberemos verificar si posee una API publica, en caso de que no, la unica manera de obtener la informacion es Scrapear.\n",
    "- Si la API es limitada en cuanto a cantidad de pedidos o bien los datos de interes no son posibles obtenerlos a trasves de esta, deberemos Scrapear.\n",
    "- En el caso de que la API cumpla con nuestros requerimientos, no necesitaremos Srapear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e17d0-9d2d-4c24-92e1-38320ccf9809",
   "metadata": {},
   "source": [
    "## 4. HTTPS\n",
    "- Es un protocolo de transferencia de hipertexto que permite a los navegadores comunicarse con los servicios web (donde se almacenan los sitios). Poniendo un ejemplo, suponiendo que nosotros realizamos un pedido a traves de nuestra computadora (peticiones o request) a un servidor, este servidor devolvera una respuesta a esta peticion (response), en caso de que este todo funcionando correctamente. Existen diferentes metodos como funciones que mediante este protocolo nos permiten obtener informacion, postearla o almacenarla.\n",
    "- En ocasiones en algunas peticiones no siempre obtenemos lo que queremos, es decir, que las peticines fallen. Existe lo que se conoce como \"codigos de status\", estos codigos nos permiten saber si nuestros pedidos fueron exitosos o si fallan cual fue el motivo. Los codigos de status pueden ser de Exito (comienzan con 2), Errores de cliente (comienzan con 4) o Errores de servidor (comienzan con 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5ae2a-9d98-4fe3-8cc5-b93b8c39020f",
   "metadata": {},
   "source": [
    "## 5. Formatos de la informacion\n",
    "- Es bastante comun que la informacion se encuentre en archivos del tipo **.csv** (comma-separated values)\n",
    "- En internet, mediante las API's u otra interraccion de datos, se utiliza el tipo **.json** (Javascript Object Notation), donde la informacion se almacena de manera similar a la estructura de un diccionario.\n",
    "- Otro tipo de formato es el tipo **.XML** donde la informacion se almacena de manera similar al equiquetado del formato HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d172cf",
   "metadata": {},
   "source": [
    "## 6. DOM\n",
    "- Document object model: Interfaz independiente del lenguaje que trata un documento XML o HMTL como una estructura de arbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176145ee",
   "metadata": {},
   "source": [
    "## 7. WebScraping con Python\n",
    "- No es la unica tecnologia que con la que se puede hacer WebScraping. Podemos usar por ejemplo, *R*, *JavaScript*, *Java*, *C++*, etc\n",
    "- Python es util en: \n",
    "    - Pedidos HTTP\n",
    "    - Parseo de la informacion: Dividir un texto en sus componentes y describir sus roles sintacticos.\n",
    "    - Automatizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba721310",
   "metadata": {},
   "source": [
    "### 7.1 Flujo de trabajo en WebScraping\n",
    "1. Le pedimos la informacion de la pagina al servidor (HTTP request).\n",
    "2. Parseamos el HTML, u otro formato, que recibamos.\n",
    "3. Procesar la informacion y guardarla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d4ce2",
   "metadata": {},
   "source": [
    "### 7.2 Parseando con Python: **Beautifulsoup**\n",
    "- Es una libreria de Python para WebScraping.\n",
    "- Se usa para extraer los datos de archivos *HTML* y *XML*\n",
    "- Esta nos permite crear un arbol de analisis, *DOM*, a partir del codigo fuente de la pagina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022e1c6",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e969d8",
   "metadata": {},
   "source": [
    "# CHAPTER I: APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0e52b",
   "metadata": {},
   "source": [
    "## I.1 Uso basico de **APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365c3c5",
   "metadata": {},
   "source": [
    "### 1.a Uso de manera directa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb372ef",
   "metadata": {},
   "source": [
    "#### Api: **SunsetAndSunrise**\n",
    "- Sirve para obtener la hora del amanecer y del ocaso de un determinado dia\n",
    "- Parametros\n",
    "    - *lat* (float): Latitud en grados decimales (obligatorio)\n",
    "    - *lng* (float): Longitud en grados decimales (obligatorio)\n",
    "    - *date* (string): Fecha en formato 'AAAA-MM-DD' (opcional. Por defecto: actual)\n",
    "- *Estructura de la query*:\n",
    "    - https://api.sunrise-sunset.org/json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2e2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los parametros de nuestra query\n",
    "latitud = -34.6\n",
    "longitud = -58.4\n",
    "fecha = '1816-07-09' # Formato: AAAA-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d151b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos libreria\n",
    "import requests\n",
    "\n",
    "# Hacemos el pedido y guarmamos la respuesta en una nueva variable\n",
    "respuesta_sunset = requests.get(f'https://api.sunrise-sunset.org/json?lat={latitud}&lng={longitud}&date={fecha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcfbce36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'sunrise': '10:58:20 AM',\n",
       "  'sunset': '8:58:27 PM',\n",
       "  'solar_noon': '3:58:24 PM',\n",
       "  'day_length': '10:00:07',\n",
       "  'civil_twilight_begin': '10:32:04 AM',\n",
       "  'civil_twilight_end': '9:24:44 PM',\n",
       "  'nautical_twilight_begin': '10:00:49 AM',\n",
       "  'nautical_twilight_end': '9:55:58 PM',\n",
       "  'astronomical_twilight_begin': '9:30:19 AM',\n",
       "  'astronomical_twilight_end': '10:26:29 PM'},\n",
       " 'status': 'OK',\n",
       " 'tzid': 'UTC'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para des-serializar el objeto (que era tipo 'HTTPResponse') y cargarlo con json\n",
    "datos_sunset = respuesta_sunset.json()\n",
    "datos_sunset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93c33536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['results', 'status', 'tzid'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tipo de dato\n",
    "type(datos_sunset)\n",
    "datos_sunset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "562a6aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: OK\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el status del pedido\n",
    "sunset_status = datos_sunset['status']\n",
    "print(f'Status: {sunset_status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64fdc6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sunrise': '10:58:20 AM',\n",
       " 'sunset': '8:58:27 PM',\n",
       " 'solar_noon': '3:58:24 PM',\n",
       " 'day_length': '10:00:07',\n",
       " 'civil_twilight_begin': '10:32:04 AM',\n",
       " 'civil_twilight_end': '9:24:44 PM',\n",
       " 'nautical_twilight_begin': '10:00:49 AM',\n",
       " 'nautical_twilight_end': '9:55:58 PM',\n",
       " 'astronomical_twilight_begin': '9:30:19 AM',\n",
       " 'astronomical_twilight_end': '10:26:29 PM'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_sunset['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0fc8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dia 1816-07-09, el sol se oculto a las 8:58:27 PM (UTC).\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver su contenido ya que son diccionarios anidados\n",
    "sunset = datos_sunset['results']['sunset']\n",
    "print(f'El dia {fecha}, el sol se oculto a las {sunset} (UTC).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe190e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterando sobre data_sunset[\"results\"]\n",
      "sunrise\n",
      "sunset\n",
      "solar_noon\n",
      "day_length\n",
      "civil_twilight_begin\n",
      "civil_twilight_end\n",
      "nautical_twilight_begin\n",
      "nautical_twilight_end\n",
      "astronomical_twilight_begin\n",
      "astronomical_twilight_end\n"
     ]
    }
   ],
   "source": [
    "# Tambien podriamos iterar sobre sus claves\n",
    "print('Iterando sobre data_sunset[\"results\"]')\n",
    "for elemento in datos_sunset['results']:\n",
    "    print(elemento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac0ce5",
   "metadata": {},
   "source": [
    "### 1.a Uso por medio de una libreria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a8260",
   "metadata": {},
   "source": [
    "#### Libreria-Api: **Wikipedia**\n",
    "- Es un *wrapper* de python facil de usar para la **API** de *Wikipedia*. Admite extraccion de textos, secciones, enlaces, categorias, traducciones, etc.\n",
    "- Repositorio: \n",
    "- Documentacion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098f433",
   "metadata": {},
   "source": [
    "#### Instalamos el paquete porque no viene por defecto\n",
    "*%pip install wikipedia-api==0.5.8 --user*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa7143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api==0.5.8 in c:\\users\\rig1\\appdata\\roaming\\python\\python312\\site-packages (0.5.8)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from wikipedia-api==0.5.8) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia-api==0.5.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8953ed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5, 8)\n"
     ]
    }
   ],
   "source": [
    "# Ahora podemos importarlo\n",
    "import wikipediaapi\n",
    "\n",
    "# Chequear version\n",
    "print(wikipediaapi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ec135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
