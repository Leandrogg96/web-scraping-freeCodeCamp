{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37aad50-4ce8-492f-9f88-d4a877de085d",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef456117-5548-4183-929c-c5059e0afb03",
   "metadata": {},
   "source": [
    "## 1. ¿Que es el Web Scrapring?\n",
    "- Web Scraping:\n",
    "    Es la practica de recopilar datos mediante un programa automatizado que consulta un servidor web. En otras palabras, un codigo que va a realizar peticiones a un determinado servidor y extraer la informacion de un tipo de dato dentro de la pagina. El tipo de informacion a extraer puede ser de cualquier tipo como por ejemplo texto, audio, imagenes, etc.\n",
    "- Web Crawling:\n",
    "    Se utiliza para indexar la informacion de una pagina mediante bots. El objetivo de esto es entrar a todos los hipervinculos o paginas dentro de una pagina y obtener toda la informacion. Un ejemplo de esto es el bot de google que indexa las paginas y asi cuando se realice una busqueda pueda realizar la recomendacion de una pagina para dicha peticion.\n",
    "- Observacion:\n",
    "    La diferencia entre ambas es que cuando realizamos WebScraping buscamos algun tipo de dato en particular, por ejemplo si lo realizamos en una pagina de venta de ropa online, intentamos obtener unicamente la descripcion de los precios de la ropa que ofrecen, por el contrario, el WebCrawling, traeremos toda la informacion de la pagina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a69ba-e01b-4616-b827-7deeeadeb6af",
   "metadata": {},
   "source": [
    "## 2. ¿Que es una API?\n",
    "API corresponde a las siglas \"Interfaz de programacion de aplicaciones\". Es un proveedor de acceso a los datos de una aplicacion, sistema operativo u otro servicio. En ocaciones las paginas o servidores proporcionan una API con el objetivo de controlar la transaccion de la informacion solicitada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937c318-c355-4f57-9c79-fcfe0f39df68",
   "metadata": {},
   "source": [
    "## 3. API vs Web Scraping\n",
    "- Las API's son mas sencillas de utilizar a la hora de recopilar informacion de manera mas limpia y a la hora de hacer pedidos especificos.\n",
    "- Cuando querramos obtener informacion de un sitio en especifico deberemos verificar si posee una API publica, en caso de que no, la unica manera de obtener la informacion es Scrapear.\n",
    "- Si la API es limitada en cuanto a cantidad de pedidos o bien los datos de interes no son posibles obtenerlos a trasves de esta, deberemos Scrapear.\n",
    "- En el caso de que la API cumpla con nuestros requerimientos, no necesitaremos Srapear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e17d0-9d2d-4c24-92e1-38320ccf9809",
   "metadata": {},
   "source": [
    "## 4. HTTPS\n",
    "- Es un protocolo de transferencia de hipertexto que permite a los navegadores comunicarse con los servicios web (donde se almacenan los sitios). Poniendo un ejemplo, suponiendo que nosotros realizamos un pedido a traves de nuestra computadora (peticiones o request) a un servidor, este servidor devolvera una respuesta a esta peticion (response), en caso de que este todo funcionando correctamente. Existen diferentes metodos como funciones que mediante este protocolo nos permiten obtener informacion, postearla o almacenarla.\n",
    "- En ocasiones en algunas peticiones no siempre obtenemos lo que queremos, es decir, que las peticines fallen. Existe lo que se conoce como \"codigos de status\", estos codigos nos permiten saber si nuestros pedidos fueron exitosos o si fallan cual fue el motivo. Los codigos de status pueden ser de Exito (comienzan con 2), Errores de cliente (comienzan con 4) o Errores de servidor (comienzan con 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5ae2a-9d98-4fe3-8cc5-b93b8c39020f",
   "metadata": {},
   "source": [
    "## 5. Formatos de la informacion\n",
    "- Es bastante comun que la informacion se encuentre en archivos del tipo **.csv** (comma-separated values)\n",
    "- En internet, mediante las API's u otra interraccion de datos, se utiliza el tipo **.json** (Javascript Object Notation), donde la informacion se almacena de manera similar a la estructura de un diccionario.\n",
    "- Otro tipo de formato es el tipo **.XML** donde la informacion se almacena de manera similar al equiquetado del formato HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d172cf",
   "metadata": {},
   "source": [
    "## 6. DOM\n",
    "- Document object model: Interfaz independiente del lenguaje que trata un documento XML o HMTL como una estructura de arbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176145ee",
   "metadata": {},
   "source": [
    "## 7. WebScraping con Python\n",
    "- No es la unica tecnologia que con la que se puede hacer WebScraping. Podemos usar por ejemplo, *R*, *JavaScript*, *Java*, *C++*, etc\n",
    "- Python es util en: \n",
    "    - Pedidos HTTP\n",
    "    - Parseo de la informacion: Dividir un texto en sus componentes y describir sus roles sintacticos.\n",
    "    - Automatizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba721310",
   "metadata": {},
   "source": [
    "### 7.1 Flujo de trabajo en WebScraping\n",
    "1. Le pedimos la informacion de la pagina al servidor (HTTP request).\n",
    "2. Parseamos el HTML, u otro formato, que recibamos.\n",
    "3. Procesar la informacion y guardarla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d4ce2",
   "metadata": {},
   "source": [
    "### 7.2 Parseando con Python: **Beautifulsoup**\n",
    "- Es una libreria de Python para WebScraping.\n",
    "- Se usa para extraer los datos de archivos *HTML* y *XML*\n",
    "- Esta nos permite crear un arbol de analisis, *DOM*, a partir del codigo fuente de la pagina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022e1c6",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e969d8",
   "metadata": {},
   "source": [
    "# CHAPTER I: APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0e52b",
   "metadata": {},
   "source": [
    "## I.1 Uso basico de **APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365c3c5",
   "metadata": {},
   "source": [
    "### 1.a Uso de manera directa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb372ef",
   "metadata": {},
   "source": [
    "#### Api: **SunsetAndSunrise**\n",
    "- Sirve para obtener la hora del amanecer y del ocaso de un determinado dia\n",
    "- Parametros\n",
    "    - *lat* (float): Latitud en grados decimales (obligatorio)\n",
    "    - *lng* (float): Longitud en grados decimales (obligatorio)\n",
    "    - *date* (string): Fecha en formato 'AAAA-MM-DD' (opcional. Por defecto: actual)\n",
    "- *Estructura de la query*:\n",
    "    - https://api.sunrise-sunset.org/json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2e2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los parametros de nuestra query\n",
    "latitud = -34.6\n",
    "longitud = -58.4\n",
    "fecha = '1816-07-09' # Formato: AAAA-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d151b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos libreria\n",
    "import requests\n",
    "\n",
    "# Hacemos el pedido y guarmamos la respuesta en una nueva variable\n",
    "respuesta_sunset = requests.get(f'https://api.sunrise-sunset.org/json?lat={latitud}&lng={longitud}&date={fecha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcfbce36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'sunrise': '10:58:20 AM',\n",
       "  'sunset': '8:58:27 PM',\n",
       "  'solar_noon': '3:58:24 PM',\n",
       "  'day_length': '10:00:07',\n",
       "  'civil_twilight_begin': '10:32:04 AM',\n",
       "  'civil_twilight_end': '9:24:44 PM',\n",
       "  'nautical_twilight_begin': '10:00:49 AM',\n",
       "  'nautical_twilight_end': '9:55:58 PM',\n",
       "  'astronomical_twilight_begin': '9:30:19 AM',\n",
       "  'astronomical_twilight_end': '10:26:29 PM'},\n",
       " 'status': 'OK',\n",
       " 'tzid': 'UTC'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para des-serializar el objeto (que era tipo 'HTTPResponse') y cargarlo con json\n",
    "datos_sunset = respuesta_sunset.json()\n",
    "datos_sunset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93c33536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['results', 'status', 'tzid'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tipo de dato\n",
    "type(datos_sunset)\n",
    "datos_sunset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "562a6aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: OK\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el status del pedido\n",
    "sunset_status = datos_sunset['status']\n",
    "print(f'Status: {sunset_status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64fdc6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sunrise': '10:58:20 AM',\n",
       " 'sunset': '8:58:27 PM',\n",
       " 'solar_noon': '3:58:24 PM',\n",
       " 'day_length': '10:00:07',\n",
       " 'civil_twilight_begin': '10:32:04 AM',\n",
       " 'civil_twilight_end': '9:24:44 PM',\n",
       " 'nautical_twilight_begin': '10:00:49 AM',\n",
       " 'nautical_twilight_end': '9:55:58 PM',\n",
       " 'astronomical_twilight_begin': '9:30:19 AM',\n",
       " 'astronomical_twilight_end': '10:26:29 PM'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_sunset['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0fc8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dia 1816-07-09, el sol se oculto a las 8:58:27 PM (UTC).\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver su contenido ya que son diccionarios anidados\n",
    "sunset = datos_sunset['results']['sunset']\n",
    "print(f'El dia {fecha}, el sol se oculto a las {sunset} (UTC).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe190e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterando sobre data_sunset[\"results\"]\n",
      "sunrise\n",
      "sunset\n",
      "solar_noon\n",
      "day_length\n",
      "civil_twilight_begin\n",
      "civil_twilight_end\n",
      "nautical_twilight_begin\n",
      "nautical_twilight_end\n",
      "astronomical_twilight_begin\n",
      "astronomical_twilight_end\n"
     ]
    }
   ],
   "source": [
    "# Tambien podriamos iterar sobre sus claves\n",
    "print('Iterando sobre data_sunset[\"results\"]')\n",
    "for elemento in datos_sunset['results']:\n",
    "    print(elemento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac0ce5",
   "metadata": {},
   "source": [
    "### 1.b Uso por medio de una libreria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a8260",
   "metadata": {},
   "source": [
    "#### Libreria-Api: **Wikipedia**\n",
    "- Es un *wrapper* de python facil de usar para la **API** de *Wikipedia*. Admite extraccion de textos, secciones, enlaces, categorias, traducciones, etc.\n",
    "- Repositorio: \n",
    "- Documentacion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098f433",
   "metadata": {},
   "source": [
    "#### Instalamos el paquete porque no viene por defecto\n",
    "*%pip install wikipedia-api==0.5.8 --user*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api==0.5.8 in c:\\users\\rig1\\appdata\\roaming\\python\\python312\\site-packages (0.5.8)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from wikipedia-api==0.5.8) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->wikipedia-api==0.5.8) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia-api==0.5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8953ed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5, 8)\n"
     ]
    }
   ],
   "source": [
    "# Ahora podemos importarlo\n",
    "import wikipediaapi\n",
    "\n",
    "# Chequear version\n",
    "print(wikipediaapi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928ec135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_programacion es un objeto de tipo: \n",
      "\n",
      "<class 'wikipediaapi.WikipediaPage'>\n"
     ]
    }
   ],
   "source": [
    "# Instaciamos la clase wikipediaapi y utilizamos el metodo Wikipedia con el parametro idioma\n",
    "IDIOMA = 'es'\n",
    "wiki_wiki = wikipediaapi.Wikipedia(IDIOMA)\n",
    "\n",
    "# Usamos el metodo page y hacemos un pedido con una palabra clave\n",
    "PALABRA_CLAVE = 'programacion'\n",
    "wikipedia_programacion = wiki_wiki.page(PALABRA_CLAVE)\n",
    "\n",
    "print(f'wikipedia_programacion es un objeto de tipo: \\n\\n{type(wikipedia_programacion)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add49fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programacion\n",
      " \n",
      "La programación es el proceso de crear un conjunto de instrucciones que le dicen a una computadora como realizar algún tipo de tarea. Pero no solo la acción de escribir un código para que la computadora o el software lo ejecute. Incluye, además, todas las tareas necesarias para que el código funcione correctamente y cumpla el objetivo para el cual se escribió.[1]​\n",
      "En la actualidad, la noción de programación se encuentra muy asociada a la creación de aplicaciones de informática y videojuegos. En este sentido, es el proceso por el cual una persona desarrolla un programa, valiéndose de una herramienta que le permita escribir el código (el cual puede estar en uno o varios lenguajes, como C++, Java y Python, entre muchos otros) y de otra que sea capaz de “traducirlo” a lo que se conoce como lenguaje de máquina, que puede \"comprender\" el microprocesador.[2]​\n"
     ]
    }
   ],
   "source": [
    "# Resumen\n",
    "print(wikipedia_programacion.title)\n",
    "print(' ')\n",
    "print(wikipedia_programacion.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958c9efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://es.wikipedia.org/wiki/Programaci%C3%B3n\n"
     ]
    }
   ],
   "source": [
    "# URL completa\n",
    "print(wikipedia_programacion.fullurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fd83a",
   "metadata": {},
   "source": [
    "## I.2 **BeautifulSoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922cf03",
   "metadata": {},
   "source": [
    "### *Documentacion oficial*: https://beautiful-soup-4.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7be35",
   "metadata": {},
   "source": [
    "### 2.a Generalidades\n",
    "- Vamos a practicar con https://scrapepark.org/spanish/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f81cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de BeautifulSoup: 4.12.3\n",
      "Version de requests: 2.32.3\n"
     ]
    }
   ],
   "source": [
    "# Versiones\n",
    "import bs4\n",
    "print('Version de BeautifulSoup:', bs4.__version__)\n",
    "print('Version de requests:', requests.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a547d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con el scraping\n",
    "\n",
    "# 1. Obtenemos el HTML\n",
    "URL_BASE = 'https://scrapepark.org/spanish/'\n",
    "pedido_obtenido = requests.get(URL_BASE)\n",
    "hmtl_obtenido = pedido_obtenido.text\n",
    "\n",
    "# 2. Parsear el HTML obtenido\n",
    "soup = BeautifulSoup(hmtl_obtenido, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "943fe991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe4d88",
   "metadata": {},
   "source": [
    "### 2.b.0 Metodo *find()*\n",
    "- Nos permite quedarnos con la informacion asociada a una etiqueta HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6d02f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>¿Por qué comprar con nosotros?</h2>\n"
     ]
    }
   ],
   "source": [
    "primer_h2 = soup.find('h2')\n",
    "print(primer_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048acfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué comprar con nosotros?\n"
     ]
    }
   ],
   "source": [
    "# Solo el texto\n",
    "print(primer_h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6c56d",
   "metadata": {},
   "source": [
    "### 2.b.1 Metodo *find_all()*\n",
    "- Busca **todos** los elementos de la pagina con esa etiqueta y devuelve una \"lista\" que los contiene (devuelve un objeto de la clase *bs4.element.ResultSet*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c747d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>¿Por qué comprar con nosotros?</h2>, <h2>\n",
      "                  #<span>Novedades</span>\n",
      "</h2>, <h2>Nuestros productos</h2>, <h2>Testimonios de clientes</h2>, <h2 class=\"heading-container\">Precios</h2>]\n"
     ]
    }
   ],
   "source": [
    "h2_todos = soup.find_all('h2')\n",
    "print(h2_todos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "864d83b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué comprar con nosotros?\n",
      "\n",
      "                  #Novedades\n",
      "\n",
      "Nuestros productos\n",
      "Testimonios de clientes\n",
      "Precios\n"
     ]
    }
   ],
   "source": [
    "# Podemos iterar sobre este objeto.\n",
    "for seccion in h2_todos:\n",
    "    print(seccion.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00c3ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué comprar con nosotros?\n",
      "#Novedades\n",
      "Nuestros productos\n",
      "Testimonios de clientes\n",
      "Precios\n"
     ]
    }
   ],
   "source": [
    "# Para mas funcionalidades: get.text()\n",
    "for seccion in h2_todos:\n",
    "    print(seccion.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a0b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>¿Por qué comprar con nosotros?</h2>]\n",
      "[<h2>¿Por qué comprar con nosotros?</h2>, <h2>\n",
      "                  #<span>Novedades</span>\n",
      "</h2>, <h2>Nuestros productos</h2>]\n"
     ]
    }
   ],
   "source": [
    "# Podemos usar el parametro 'limit' para indicar la cantidad de elementos a devolver\n",
    "h2_uno_solo = soup.find_all('h2', limit=1)\n",
    "print(h2_uno_solo)\n",
    "\n",
    "h2_tres = soup.find_all('h2', limit=3)\n",
    "print(h2_tres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc761aab",
   "metadata": {},
   "source": [
    "### 2.c Utilizando atributos de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e62e3877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"heading-container heading-center\" id=\"about\">\n",
      "<h2>¿Por qué comprar con nosotros?</h2>\n",
      "</div>\n",
      "\n",
      "<div class=\"heading-container heading-center\" id=\"products\">\n",
      "<h2>Nuestros productos</h2>\n",
      "</div>\n",
      "\n",
      "<div class=\"heading-container heading-center\">\n",
      "<h3>Suscríbete para obtener descuentos y ofertas</h3>\n",
      "</div>\n",
      "\n",
      "<div class=\"heading-container heading-center\">\n",
      "<h2>Testimonios de clientes</h2>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clase\n",
    "divs = soup.find_all('div', class_= 'heading-container heading-center')\n",
    "\n",
    "for div in divs:\n",
    "    print(div)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d03561",
   "metadata": {},
   "source": [
    "### 2.d Podemos descargar imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09a401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"Parque de patinaje\" src=\"../images/slider-bg.jpg\"/>\n",
      "<img alt=\"Patineta\" src=\"../images/p2.jpg\"/>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<img alt=\"ScrapePark.org Logo\" src=\"../images/logo.svg\" width=\"250\"/>,\n",
       " <img alt=\"Parque de patinaje\" src=\"../images/slider-bg.jpg\"/>,\n",
       " <img alt=\"Variedad de patinetas en nuestra tienda\" src=\"../images/arrival-bg-store.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p1.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p2.jpg\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p3.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p4.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p5.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p6.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p7.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p8.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p9.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p10.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p11.png\"/>,\n",
       " <img alt=\"Patineta\" src=\"../images/p12.png\"/>,\n",
       " <img alt=\"Cliente\" src=\"../images/client-one.png\"/>,\n",
       " <img alt=\"Cliente\" src=\"../images/client-two.png\"/>,\n",
       " <img alt=\"Cliente\" src=\"../images/client-three.png\"/>,\n",
       " <iframe src=\"table.html\" title=\"table_iframe\"></iframe>,\n",
       " <img alt=\"Logo de ScrapePark.org\" src=\"../images/logo.svg\" width=\"210\"/>,\n",
       " <img alt=\"Logo de freeCodeCamp\" class=\"freecodecamp-logo\" src=\".././images/freecodecamp-logo.png\"/>,\n",
       " <script src=\"../js/jquery-3.4.1.min.js\"></script>,\n",
       " <script src=\"../js/popper.min.js\"></script>,\n",
       " <script src=\"../js/bootstrap.js\"></script>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todas las etiquetas que tengan el atributo \"src\"\n",
    "src_todos = soup.find_all(src=True)\n",
    "\n",
    "for elemento in src_todos:\n",
    "    if elemento['src'].endswith(\".jpg\"):\n",
    "        print(elemento)\n",
    "\n",
    "src_todos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472d905",
   "metadata": {},
   "source": [
    "### 2.e Ejercicio: Descargar todas las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3510588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../images/arrival-bg-store.png\n",
      "../images/p1.png\n",
      "../images/p3.png\n",
      "../images/p4.png\n",
      "../images/p5.png\n",
      "../images/p6.png\n",
      "../images/p7.png\n",
      "../images/p8.png\n",
      "../images/p9.png\n",
      "../images/p10.png\n",
      "../images/p11.png\n",
      "../images/p12.png\n",
      "../images/client-one.png\n",
      "../images/client-two.png\n",
      "../images/client-three.png\n",
      ".././images/freecodecamp-logo.png\n"
     ]
    }
   ],
   "source": [
    "url_imagenes = []\n",
    "\n",
    "for i, imagen in enumerate(src_todos):\n",
    "\n",
    "    if imagen['src'].endswith('png'):\n",
    "        print(imagen['src'])\n",
    "        r = requests.get(f'https://scrapepark.org/spanish/{imagen['src']}')\n",
    "\n",
    "        with open(f'imagen {i}.png', 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0cf87",
   "metadata": {},
   "source": [
    "### 2.f Otros casos de uso: *iframe* y *table*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b31c3",
   "metadata": {},
   "source": [
    "#### 2.f.1 Tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3880eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Longboard', '$80', '$85', '$90', '$62', '$150']\n"
     ]
    }
   ],
   "source": [
    "# Informacion de tablas\n",
    "URL_BASE = 'https://scrapepark.org/spanish/'\n",
    "URL_TABLA = soup.find_all('iframe')[0]['src']\n",
    "\n",
    "request_tabla = requests.get(f'{URL_BASE}/{URL_TABLA}')\n",
    "\n",
    "hmtl_tabla = request_tabla.text\n",
    "soup_tabla = BeautifulSoup(hmtl_tabla, 'html.parser')\n",
    "soup_tabla.find('table')\n",
    "\n",
    "productos_faltantes = soup_tabla.find_all(['th', 'td'], attrs={'style': 'color: red;'})\n",
    "productos_faltantes = [talle.text for talle in productos_faltantes]\n",
    "\n",
    "print(productos_faltantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757345a",
   "metadata": {},
   "source": [
    "### 2.g Datos de productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d332dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producto: Patineta nueva1  | Precio: 75\n",
      "Producto: Patineta usada2  | Precio: 80\n",
      "Producto: Patineta nueva3  | Precio: 68\n",
      "Producto: Patineta usada4  | Precio: 70\n",
      "Producto: Patineta nueva5  | Precio: 75\n",
      "Producto: Patineta nueva6  | Precio: 58\n",
      "Producto: Patineta nueva7  | Precio: 80\n",
      "Producto: Patineta nueva8  | Precio: 35\n",
      "Producto: Patineta nueva9  | Precio: 165\n",
      "Producto: Patineta usada10 | Precio: 54\n",
      "Producto: Patineta usada11 | Precio: 99\n",
      "Producto: Patineta nueva12 | Precio: 110\n"
     ]
    }
   ],
   "source": [
    "divs = soup.find_all('div', class_='detail-box')\n",
    "productos = []\n",
    "precios = []\n",
    "\n",
    "for div in divs:\n",
    "    if (div.h6 is not None) and ('Patineta' in div.h5.text):\n",
    "        producto = div.h5.get_text(strip=True)\n",
    "        precio = div.h6.get_text(strip=True).replace('$', '')\n",
    "        # Se puede agregar filtros\n",
    "        print(f'Producto: {producto:<16} | Precio: {precio}')\n",
    "        productos.append(producto)\n",
    "        precios.append(precio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5a864",
   "metadata": {},
   "source": [
    "### 2.h Cambios que dependen de la URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbaeeecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://scrapepark.org/spanish/contact1\n",
      "Texto que cambia entre páginas en contacto 1 :)\n",
      "URL: https://scrapepark.org/spanish/contact2\n",
      "Texto que cambia entre páginas en contacto 2 :)\n"
     ]
    }
   ],
   "source": [
    "URL_BASE_II = \"https://scrapepark.org/spanish/contact\"\n",
    "\n",
    "for i in range(1,3):\n",
    "    URL_FINAL = f'{URL_BASE_II}{i}'\n",
    "    print(f'URL: {URL_FINAL}')\n",
    "    r = requests.get(URL_FINAL)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    print(soup.h5.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e728e50",
   "metadata": {},
   "source": [
    "### 2.i Datos que no sabemos en que parte de la pagina se encuentran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99340a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[': 4-444-4444']\n"
     ]
    }
   ],
   "source": [
    "# Expresiones regulares\n",
    "import re\n",
    "\n",
    "# 1. Obtener el HTML\n",
    "URL_BASE_III = 'https://scrapepark.org/spanish'\n",
    "get_order = requests.get(URL_BASE_III)\n",
    "get_html = get_order.text\n",
    "\n",
    "# 2. Parsear ese HTML\n",
    "soup = BeautifulSoup(get_html, 'html.parser')\n",
    "\n",
    "celphones = soup.find_all(string=re.compile(r'\\d+-\\d+-\\d+'))\n",
    "print(celphones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c73cc",
   "metadata": {},
   "source": [
    "### 2.j Moviendonos por el arbol\n",
    "- Para saber mas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea6874b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['© 2022 ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyrights = soup.find_all(string=re.compile('©'))\n",
    "copyrights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b586ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>© 2022 <span>Todos los derechos reservados</span>.\n",
       "        <a href=\"https://html.design/\" rel=\"noopener noreferrer\" target=\"_blank\">Creado con Free Html Templates</a>.\n",
       "      </p>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_copyright = copyrights[0]\n",
    "first_copyright.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6eb12a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul>\n",
       " <li><a href=\"#\">Inicio</a></li>\n",
       " <li><a href=\"#\">Acerca</a></li>\n",
       " <li><a href=\"#\">Servicios</a></li>\n",
       " <li><a href=\"#\">Testimonios</a></li>\n",
       " <li><a href=\"#\">Contacto</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otro ejemplo con elementos al mismo nivel\n",
    "menu = soup.find(string=re.compile('MENÚ'))\n",
    "menu.parent\n",
    "\n",
    "menu.parent.find_next_siblings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00c673",
   "metadata": {},
   "source": [
    "### 2.k Comentario sobre excepciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0a431cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENÚ\n",
      "© 2022 \n",
      "El string \"carpincho\" no fue encontrado.\n",
      "Patineta nueva\n"
     ]
    }
   ],
   "source": [
    "strings_for_search = ['MENÚ', '©', 'carpincho', 'Patineta']\n",
    "\n",
    "for string in strings_for_search:\n",
    "    try:\n",
    "        result = soup.find(string=re.compile(string))\n",
    "        print(result.text)\n",
    "    except AttributeError:\n",
    "        print(f'El string \"{string}\" no fue encontrado.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910f1c8",
   "metadata": {},
   "source": [
    "### 2.l Almacenamiento de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18a4ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "productos.insert(0, 'productos')\n",
    "precios.insert(0,'precios')\n",
    "datos = dict(zip(productos, precios))\n",
    "with open('datos.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(datos.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc7fa9",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424ab07",
   "metadata": {},
   "source": [
    "# CHAPTER II: Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da591f2",
   "metadata": {},
   "source": [
    "## Ejercicio I:\n",
    "- 1.1 Las patinetas que salgan menos que $68.\n",
    "- 1.2 Las patinetas que en su nombre tengan un numero mayor a 3.\n",
    "- 1.3 Traer cualquier texto de la pagina que tenga la palabra *descuento* u *oferta*\n",
    "- 1.4 Genrar un archivo *.csv* con dos columnas. Una conteniendo el nombre del cliente y ota su testimonio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5b7d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
